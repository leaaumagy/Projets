{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a161c6-9f4d-4411-9656-309cc1c1091c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Scraping web site : Hacker News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1465d72d-40c0-4126-8dc9-90a36960bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leaaumagy/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541b155-2bf4-4ee6-8e40-acfe19954aed",
   "metadata": {},
   "source": [
    "## **2. Schematise the HTML structure by drawing tree block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c653f-ceee-418b-923b-7ed0cbcd9469",
   "metadata": {},
   "source": [
    "This command opens the link of the file I inserted on my google drive.\n",
    "It is also available in the .zip file if there is a problem opening it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38ae90d-f549-46e7-997d-097e0dcf6f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('https://drive.google.com/file/d/1GUOLmITXO78E2WoPkhtNNYQj-7r2FLwn/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cab98d-15c7-4826-83c6-9b01f0a3ad57",
   "metadata": {},
   "source": [
    "## **3. Get the HTML page in Python using the requests library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899e1d87-50c6-4985-9062-c85fc973b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_website = requests.get('https://news.ycombinator.com/')\n",
    "soup = BeautifulSoup(html_website.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768680-f781-4811-bd03-69ac4537425e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Start parsing the page with Beautiful Soup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b123e7-241a-49e5-9d10-1a533e9e7474",
   "metadata": {},
   "source": [
    "### **b) Print all the title on the homepage (30 names)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe5c394-8a6e-46ae-aa78-b83f5fd69200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The computers used to do 3D animation for Final Fantasy VII in 1996',\n",
       " 'Ruby 3.2 preview 1 with support for WASM compilation',\n",
       " 'What software engineers can learn from the rapid collapse of Fast',\n",
       " \"Show HN: Redditle.com – For those of us who add 'Reddit' to every Google search\",\n",
       " 'The ever-increasing walled-gardeness of Twitter',\n",
       " 'TypeScript as fast as Rust: TypeScript++',\n",
       " 'Newly Measured Particle Seems Heavy Enough to Break Known Physics',\n",
       " 'The Muse (YC W12) Is Hiring a Senior Platform Engineer (Learn More)',\n",
       " 'The 0.5 MB of nothing in all Apple Music files (2020)',\n",
       " 'I stopped advertising and nothing happened',\n",
       " 'ACM Opens First 50 Years Backfile',\n",
       " 'Denonia: The First Malware Specifically Targeting AWS Lambda',\n",
       " 'Tree-sitter grammar for org-mode',\n",
       " 'A detailed look at the S-300P anti-aircraft missile system',\n",
       " 'Microplastics found in live human lung tissue',\n",
       " 'From context collapse to content collapse',\n",
       " 'Canada to ban foreigners from buying homes',\n",
       " 'Mantle – Serverless Maps Using Lambda or Cloudflare Workers',\n",
       " 'Show HN: Codestat.dev – Stats from 2M open-source repositories',\n",
       " 'Use Git tactically',\n",
       " 'Raspberry Pi update removes the default user',\n",
       " 'Ask HN: Have you ever switched cloud?',\n",
       " 'The physicalization of metamathematics and the implications for its foundations',\n",
       " 'Show HN: Pipedream 2.0 – AWS Lambda + Zapier alternative',\n",
       " 'Purple dye extracted from sea snails made the Phoenicians rich traders (2020)',\n",
       " 'Redesigning an app, one day a week at a time',\n",
       " 'Unit Testing is Overrated (2020)',\n",
       " 'Using Windows after 15 years on Linux',\n",
       " 'Advantages of Monorepos (2015)',\n",
       " 'System/360 Announcement (1964)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlelink = []\n",
    "for title in soup.find_all('a', {'class':'titlelink'}) :\n",
    "    titlelink.append(title.text)\n",
    "titlelink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b380af67-affa-45d2-9218-67eeb61a8aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check that we have the 30 titles of each tag\n",
    "len(titlelink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba86bd3-97be-4ebf-b359-991e5dcded53",
   "metadata": {},
   "source": [
    "## **5. Structure your information by creating a dataclass named Post containing all the relevant information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b60f44-a619-48a0-8865-4854b1b6c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of the Post class\n",
    "#Each class is defaulted to 'nan' except for the rank and the post name  \n",
    "\n",
    "class Post():\n",
    "    def __init__(self, rank, name, site = np.nan, site_url = np.nan, point = np.nan, author = np.nan, date_published = np.nan, number_comments = np.nan):\n",
    "        self.rank = rank\n",
    "        self.name = name\n",
    "        self.site = site\n",
    "        self.site_url = site_url\n",
    "        self.point = point\n",
    "        self.author = author\n",
    "        self.date_published = date_published\n",
    "        self.number_comments = number_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24906f3f-4d43-4453-add8-4c77b6f0bd6c",
   "metadata": {},
   "source": [
    "## **6. Parse the following informations: name , points , author , data published , number of comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fc8054-1e84-4b23-b54c-fd84f1234903",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tr = soup.find('table',{'class':'itemlist'}).find_all('tr')\n",
    "\n",
    "#size is defined as the size of the 'list_tr' - 2 because the last two 'tr' are not useful\n",
    "size = len(list_tr)-2\n",
    "\n",
    "all_posts = []\n",
    "\n",
    "\n",
    "for index in range(0,size,3):\n",
    "    line1 = list_tr[index] \n",
    "    line2 = line1.next_sibling   \n",
    "    post = Post(\n",
    "        rank = line1.find('span',{'class':'rank'}).text.replace('.',''),    \n",
    "        name = line1.find('a',{'class':'titlelink'}).text )\n",
    "\n",
    "#We have a range with a step of 3 because the information for each article is located every three 'tr' tags.\n",
    "#line1 represents the first 'tr' among the blocks of 3 where the title, the name of the site and its url are found\n",
    "#line2 represents the second 'tr' among the blocks of 3 where the points, the author and the date of publication are found \n",
    "    \n",
    "#We perform a condition on the other classes\n",
    "#If the class has content then the class takes it\n",
    "    \n",
    "    #Get the name of the website\n",
    "    element = line1.find('a',{'class':'titlelink'})  \n",
    "    \n",
    "    if element :\n",
    "        post.site_url = element.get('href')\n",
    "        \n",
    "    element = line1.find('span',{'class':'sitestr'})\n",
    "    if element :\n",
    "        post.site = element.text\n",
    "        \n",
    "    element = line2.find('span',{'class':'score'})\n",
    "    if element:\n",
    "        post.point=element.text.replace(' points','')\n",
    "        \n",
    "    element = line2.find('a',{'class':'hnuser'})\n",
    "    if element:\n",
    "        post.author=element.text\n",
    "        \n",
    "    element = line2.find('span',{'class':'age'})\n",
    "    if element:\n",
    "        post.date_published=element.get('title')\n",
    "        \n",
    "    element = line2.find(string=re.compile('comment'))\n",
    "    if element:\n",
    "        post.number_comments=element.replace('comments','')\n",
    "        \n",
    "    #Save the current post    \n",
    "    all_posts.append(vars(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e8826-d2e3-45cd-b7ff-f6cbfae13dee",
   "metadata": {},
   "source": [
    "## **7. Write the data in a CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ea1043-7e6a-4d13-9b72-7f96025d3991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>site</th>\n",
       "      <th>site_url</th>\n",
       "      <th>point</th>\n",
       "      <th>author</th>\n",
       "      <th>date_published</th>\n",
       "      <th>number_comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The computers used to do 3D animation for Fina...</td>\n",
       "      <td>lunduke.substack.com</td>\n",
       "      <td>https://lunduke.substack.com/p/the-computers-u...</td>\n",
       "      <td>393</td>\n",
       "      <td>marcobambini</td>\n",
       "      <td>2022-04-07T16:55:20</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruby 3.2 preview 1 with support for WASM compi...</td>\n",
       "      <td>ruby-lang.org</td>\n",
       "      <td>https://www.ruby-lang.org/en/news/2022/04/03/r...</td>\n",
       "      <td>210</td>\n",
       "      <td>pvsukale3</td>\n",
       "      <td>2022-04-07T17:38:00</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What software engineers can learn from the rap...</td>\n",
       "      <td>pragmaticengineer.com</td>\n",
       "      <td>https://newsletter.pragmaticengineer.com/p/the...</td>\n",
       "      <td>220</td>\n",
       "      <td>gregdoesit</td>\n",
       "      <td>2022-04-07T17:23:16</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Show HN: Redditle.com – For those of us who ad...</td>\n",
       "      <td>redditle.com</td>\n",
       "      <td>https://redditle.com</td>\n",
       "      <td>261</td>\n",
       "      <td>greentfrapp</td>\n",
       "      <td>2022-04-07T16:33:15</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The ever-increasing walled-gardeness of Twitter</td>\n",
       "      <td>annoying.technology</td>\n",
       "      <td>https://annoying.technology/posts/e6901c0ea272...</td>\n",
       "      <td>87</td>\n",
       "      <td>matrixagent</td>\n",
       "      <td>2022-04-07T19:10:48</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "rank                                                      \n",
       "1     The computers used to do 3D animation for Fina...   \n",
       "2     Ruby 3.2 preview 1 with support for WASM compi...   \n",
       "3     What software engineers can learn from the rap...   \n",
       "4     Show HN: Redditle.com – For those of us who ad...   \n",
       "5       The ever-increasing walled-gardeness of Twitter   \n",
       "\n",
       "                       site  \\\n",
       "rank                          \n",
       "1      lunduke.substack.com   \n",
       "2             ruby-lang.org   \n",
       "3     pragmaticengineer.com   \n",
       "4              redditle.com   \n",
       "5       annoying.technology   \n",
       "\n",
       "                                               site_url point        author  \\\n",
       "rank                                                                          \n",
       "1     https://lunduke.substack.com/p/the-computers-u...   393  marcobambini   \n",
       "2     https://www.ruby-lang.org/en/news/2022/04/03/r...   210     pvsukale3   \n",
       "3     https://newsletter.pragmaticengineer.com/p/the...   220    gregdoesit   \n",
       "4                                  https://redditle.com   261   greentfrapp   \n",
       "5     https://annoying.technology/posts/e6901c0ea272...    87   matrixagent   \n",
       "\n",
       "           date_published number_comments  \n",
       "rank                                       \n",
       "1     2022-04-07T16:55:20            189   \n",
       "2     2022-04-07T17:38:00             46   \n",
       "3     2022-04-07T17:23:16            144   \n",
       "4     2022-04-07T16:33:15            138   \n",
       "5     2022-04-07T19:10:48             56   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(all_posts).set_index('rank')\n",
    "data.to_csv('posts.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22af29e-e864-4330-8fd9-4f384f333469",
   "metadata": {},
   "source": [
    "## **8. Write code to parse the 5 first pages of HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb7d9003-0338-4e03-88a8-6a8bf7b59706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFivePages(filename='posts_5_pages.csv'):\n",
    "    all_posts = []\n",
    "\n",
    "#extraction of pages 1 to 5\n",
    "\n",
    "    for page in range(1,6):\n",
    "        html_website = requests.get(f'https://news.ycombinator.com?p={page}')\n",
    "        soup = BeautifulSoup(html_website.text, 'html.parser')\n",
    "        list_tr = soup.find('table',{'class':'itemlist'}).find_all('tr')\n",
    "        size = len(list_tr)-2\n",
    "        \n",
    "        \n",
    "        #rewriting the code for page 1 that will run on pages 1 to 5\n",
    "\n",
    "        for index in range(0,size,3):\n",
    "            line1 = list_tr[index]\n",
    "            line2 = line1.next_sibling\n",
    "            post = Post(\n",
    "                rank = line1.find('span',{'class':'rank'}).text.replace('.',''),\n",
    "                name = line1.find('a',{'class':'titlelink'}).text)\n",
    "            \n",
    "            #Get the name of the website\n",
    "            element = line1.find('a',{'class':'titlelink'})\n",
    "           \n",
    "            #If element is not none\n",
    "            if element :\n",
    "                post.site_url = element.get('href')\n",
    "\n",
    "            element = line1.find('span',{'class':'sitestr'})\n",
    "            if element :\n",
    "                post.site = element.text\n",
    "\n",
    "            element = line2.find('span',{'class':'score'})\n",
    "            if element:\n",
    "                post.point = element.text.replace(' points','')\n",
    "\n",
    "            element = line2.find('a',{'class':'hnuser'})\n",
    "            if element:\n",
    "                post.author = element.text\n",
    "\n",
    "            element = line2.find('span',{'class':'age'})\n",
    "            if element:\n",
    "                post.date_published = element.get('title')\n",
    "\n",
    "            element = line2.find(string = re.compile('comment'))\n",
    "            if element:\n",
    "                post.number_comments = element.replace('comments','')\n",
    "                \n",
    "            #Save the current post    \n",
    "            all_posts.append(vars(post))\n",
    "          \n",
    "        \n",
    "    #creation of the dataframe and the CSV file for the 5 pages\n",
    "    data = pd.DataFrame(all_posts)\n",
    "    data = data.set_index('rank')\n",
    "    data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5ddd84-a647-4e52-bf56-2db42b81a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parseFivePages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9cfab-4e69-4bf2-8f31-d970576d0d07",
   "metadata": {},
   "source": [
    "## **9. Think about a way to run your code every hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fed2d-048e-4571-9208-d9e4e5102141",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hours  = 60 * 60  \n",
    "\n",
    "#one_hours is defined in seconds\n",
    "#in an hour there are 60 minutes that have 60 seconds\n",
    "\n",
    "while True:\n",
    "    sleep(one_hours) \n",
    "    parseFivePages('every_hours.csv') #creation of the CSV file witch run each hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
