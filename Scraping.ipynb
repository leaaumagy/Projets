{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a161c6-9f4d-4411-9656-309cc1c1091c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Scraping web site : Hacker News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465d72d-40c0-4126-8dc9-90a36960bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541b155-2bf4-4ee6-8e40-acfe19954aed",
   "metadata": {},
   "source": [
    "## **2. Schematise the HTML structure by drawing tree block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c653f-ceee-418b-923b-7ed0cbcd9469",
   "metadata": {},
   "source": [
    "This command opens the link of the file I inserted on my google drive.\n",
    "It is also available in the .zip file if there is a problem opening it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ae90d-f549-46e7-997d-097e0dcf6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open('https://drive.google.com/file/d/1GUOLmITXO78E2WoPkhtNNYQj-7r2FLwn/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cab98d-15c7-4826-83c6-9b01f0a3ad57",
   "metadata": {},
   "source": [
    "## **3. Get the HTML page in Python using the requests library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e1d87-50c6-4985-9062-c85fc973b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_website = requests.get('https://news.ycombinator.com/')\n",
    "soup = BeautifulSoup(html_website.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768680-f781-4811-bd03-69ac4537425e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **4. Start parsing the page with Beautiful Soup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b123e7-241a-49e5-9d10-1a533e9e7474",
   "metadata": {},
   "source": [
    "### **b) Print all the title on the homepage (30 names)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5c394-8a6e-46ae-aa78-b83f5fd69200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titlelink = []\n",
    "for title in soup.find_all('a', {'class':'titlelink'}) :\n",
    "    titlelink.append(title.text)\n",
    "titlelink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380af67-affa-45d2-9218-67eeb61a8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check that we have the 30 titles of each tag\n",
    "len(titlelink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba86bd3-97be-4ebf-b359-991e5dcded53",
   "metadata": {},
   "source": [
    "## **5. Structure your information by creating a dataclass named Post containing all the relevant information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b60f44-a619-48a0-8865-4854b1b6c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation of the Post class\n",
    "#Each class is defaulted to 'nan' except for the rank and the post name  \n",
    "\n",
    "class Post():\n",
    "    def __init__(self, rank, name, site = np.nan, site_url = np.nan, point = np.nan, author = np.nan, date_published = np.nan, number_comments = np.nan):\n",
    "        self.rank = rank\n",
    "        self.name = name\n",
    "        self.site = site\n",
    "        self.site_url = site_url\n",
    "        self.point = point\n",
    "        self.author = author\n",
    "        self.date_published = date_published\n",
    "        self.number_comments = number_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24906f3f-4d43-4453-add8-4c77b6f0bd6c",
   "metadata": {},
   "source": [
    "## **6. Parse the following informations: name , points , author , data published , number of comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc8054-1e84-4b23-b54c-fd84f1234903",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tr = soup.find('table',{'class':'itemlist'}).find_all('tr')\n",
    "\n",
    "#size is defined as the size of the 'list_tr' - 2 because the last two 'tr' are not useful\n",
    "size = len(list_tr)-2\n",
    "\n",
    "all_posts = []\n",
    "\n",
    "\n",
    "for index in range(0,size,3):\n",
    "    line1 = list_tr[index] \n",
    "    line2 = line1.next_sibling   \n",
    "    post = Post(\n",
    "        rank = line1.find('span',{'class':'rank'}).text.replace('.',''),    \n",
    "        name = line1.find('a',{'class':'titlelink'}).text )\n",
    "\n",
    "#We have a range with a step of 3 because the information for each article is located every three 'tr' tags.\n",
    "#line1 represents the first 'tr' among the blocks of 3 where the title, the name of the site and its url are found\n",
    "#line2 represents the second 'tr' among the blocks of 3 where the points, the author and the date of publication are found \n",
    "    \n",
    "#We perform a condition on the other classes\n",
    "#If the class has content then the class takes it\n",
    "    \n",
    "    #Get the name of the website\n",
    "    element = line1.find('a',{'class':'titlelink'})  \n",
    "    \n",
    "    if element :\n",
    "        post.site_url = element.get('href')\n",
    "        \n",
    "    element = line1.find('span',{'class':'sitestr'})\n",
    "    if element :\n",
    "        post.site = element.text\n",
    "        \n",
    "    element = line2.find('span',{'class':'score'})\n",
    "    if element:\n",
    "        post.point=element.text.replace(' points','')\n",
    "        \n",
    "    element = line2.find('a',{'class':'hnuser'})\n",
    "    if element:\n",
    "        post.author=element.text\n",
    "        \n",
    "    element = line2.find('span',{'class':'age'})\n",
    "    if element:\n",
    "        post.date_published=element.get('title')\n",
    "        \n",
    "    element = line2.find(string=re.compile('comment'))\n",
    "    if element:\n",
    "        post.number_comments=element.replace('comments','')\n",
    "        \n",
    "    #Save the current post    \n",
    "    all_posts.append(vars(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e8826-d2e3-45cd-b7ff-f6cbfae13dee",
   "metadata": {},
   "source": [
    "## **7. Write the data in a CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea1043-7e6a-4d13-9b72-7f96025d3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(all_posts).set_index('rank')\n",
    "data.to_csv('posts.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22af29e-e864-4330-8fd9-4f384f333469",
   "metadata": {},
   "source": [
    "## **8. Write code to parse the 5 first pages of HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d9003-0338-4e03-88a8-6a8bf7b59706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFivePages(filename='posts_5_pages.csv'):\n",
    "    all_posts = []\n",
    "\n",
    "#extraction of pages 1 to 5\n",
    "\n",
    "    for page in range(1,6):\n",
    "        html_website = requests.get(f'https://news.ycombinator.com?p={page}')\n",
    "        soup = BeautifulSoup(html_website.text, 'html.parser')\n",
    "        list_tr = soup.find('table',{'class':'itemlist'}).find_all('tr')\n",
    "        size = len(list_tr)-2\n",
    "        \n",
    "        \n",
    "        #rewriting the code for page 1 that will run on pages 1 to 5\n",
    "\n",
    "        for index in range(0,size,3):\n",
    "            line1 = list_tr[index]\n",
    "            line2 = line1.next_sibling\n",
    "            post = Post(\n",
    "                rank = line1.find('span',{'class':'rank'}).text.replace('.',''),\n",
    "                name = line1.find('a',{'class':'titlelink'}).text)\n",
    "            \n",
    "            #Get the name of the website\n",
    "            element = line1.find('a',{'class':'titlelink'})\n",
    "           \n",
    "            #If element is not none\n",
    "            if element :\n",
    "                post.site_url = element.get('href')\n",
    "\n",
    "            element = line1.find('span',{'class':'sitestr'})\n",
    "            if element :\n",
    "                post.site = element.text\n",
    "\n",
    "            element = line2.find('span',{'class':'score'})\n",
    "            if element:\n",
    "                post.point = element.text.replace(' points','')\n",
    "\n",
    "            element = line2.find('a',{'class':'hnuser'})\n",
    "            if element:\n",
    "                post.author = element.text\n",
    "\n",
    "            element = line2.find('span',{'class':'age'})\n",
    "            if element:\n",
    "                post.date_published = element.get('title')\n",
    "\n",
    "            element = line2.find(string = re.compile('comment'))\n",
    "            if element:\n",
    "                post.number_comments = element.replace('comments','')\n",
    "                \n",
    "            #Save the current post    \n",
    "            all_posts.append(vars(post))\n",
    "          \n",
    "        \n",
    "    #creation of the dataframe and the CSV file for the 5 pages\n",
    "    data = pd.DataFrame(all_posts)\n",
    "    data = data.set_index('rank')\n",
    "    data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ddd84-a647-4e52-bf56-2db42b81a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parseFivePages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9cfab-4e69-4bf2-8f31-d970576d0d07",
   "metadata": {},
   "source": [
    "## **9. Think about a way to run your code every hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fed2d-048e-4571-9208-d9e4e5102141",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hours  = 60 * 60  \n",
    "\n",
    "#one_hours is defined in seconds\n",
    "#in an hour there are 60 minutes that have 60 seconds\n",
    "\n",
    "while True:\n",
    "    sleep(one_hours) \n",
    "    parseFivePages('every_hours.csv') #creation of the CSV file witch run each hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
